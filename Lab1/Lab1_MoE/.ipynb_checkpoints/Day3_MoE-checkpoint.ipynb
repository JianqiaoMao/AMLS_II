{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 3: Mixtures of Experts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how mixtures of experts can be used to boost performance.\n",
    "\n",
    "The objective of this lab is to classify images from Cifar10 (https://www.cs.toronto.edu/~kriz/cifar.html) to one of ten classes: {0: airplane, 1: automobile, 2: bird, 3: cat, 4: deer, 5: dog, 6: frog, 7: horse, 8: ship, 9: truck}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Cifar10](cifar10_resize.png )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifically, a gating function is trained to pass examples to two experts which are trained separatly, where one expert is trained to classify images within the \"natural image\" category (e.g. cat, dog, etc) and another to classify images within the  \"artificial image\" category (e.g. plane, car). The experts are then used to boost the performance of a baseline architecture that classifies image to one of the 10 classes.\n",
    "\n",
    "Specifically, the mixture is built in the following order:\n",
    "1. A single model is trained to to classify all 10 classes. (This is included in the mixture, and is also our evaluation benchmark)\n",
    "\n",
    "2. An expert gating function is trained to recognise whether an image is of an artificial or  natural subject.\n",
    "\n",
    "3. An artificial expert is trained to classify artificial objects that have a label in {0, 1, 8, 9}.\n",
    "\n",
    "4. A natural expert is trained to classify natural objects that have a label in  {2, 3, 4, 5, 6, 7}.\n",
    "\n",
    "5. A gating function is trained to determine the contribution of the experts and the contribution of the baseline architecture to the final output.\n",
    "\n",
    "6. The mixture is built as illustrated in the figure below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](moe_architecture_illus.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, Flatten, BatchNormalization, Input\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import concatenate, Lambda, Reshape\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "from keras.datasets import cifar10\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.utils import plot_model\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import pydot\n",
    "from IPython.display import SVG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters (not to be changed)\n",
    "orig_classes = 10 ; gate0_classes = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixture Parameters\n",
    "\n",
    "You can try changing the mixture parameters in the following piece of code, when doing so, consider the following:\n",
    "\n",
    "1 - Increasing the number of epochs increases the fit to training data, at some point this should cause over-fitting. Conversly, setting it low should cause under-fitting.\n",
    "\n",
    "2 - Increasing the number of training examples increases the number of learnable features.\n",
    "\n",
    "3 - Using a large model for different classifiers increases their capacity to learn. This increases the amount of required epochs for training, and also increases the risk of over-fitting.\n",
    "\n",
    "By changing the parameters, the performance of the mixture of experts and the baseline classifier should change accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of training/testing examples per batch\n",
    "batch_size = 50\n",
    "\n",
    "# Training epochs. A higher number of epochs corresponds to \"more fitting to training data\"\n",
    "epochs = 1\n",
    "\n",
    "# Number of training/testing examples to use\n",
    "train_examples = 5000 # Max is 50000\n",
    "test_examples = 1000   # Max is 5000\n",
    "\n",
    "# Large/small model flags. Set to true to change a classifier to \"large\"\n",
    "use_large_experts = False\n",
    "use_large_gating_mlp = False\n",
    "use_large_baseline_classifier = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# delete previous model checkpoints\n",
    "import shutil\n",
    "shutil.rmtree('gate0Cifar10', ignore_errors=True)\n",
    "shutil.rmtree('moe3Cifar10', ignore_errors=True)\n",
    "shutil.rmtree('natureCifar10', ignore_errors=True)\n",
    "shutil.rmtree('baseCifar10', ignore_errors=True)\n",
    "shutil.rmtree('artCifar10', ignore_errors=True)\n",
    "\n",
    "# get the newest model file within a directory\n",
    "def getNewestModel(model, dirname):\n",
    "    from glob import glob\n",
    "    target = os.path.join(dirname, '*')\n",
    "    files = [(f, os.path.getmtime(f)) for f in glob(target)]\n",
    "    if len(files) == 0:\n",
    "        return model\n",
    "    else:\n",
    "        newestModel = sorted(files, key=lambda files: files[1])[-1]\n",
    "        model.load_weights(newestModel[0])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load dataset  ; X: input images,  Y: class label ground truth\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "x_train = x_train[:train_examples] ; x_test = x_test[:test_examples]\n",
    "y_train = y_train[:train_examples] ; y_test = y_test[:test_examples]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prepare x dataset\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert class vectors to binary class matrices\n",
    "y_train0 = keras.utils.to_categorical(y_train, orig_classes)\n",
    "y_test0 = keras.utils.to_categorical(y_test, orig_classes)\n",
    "\n",
    "print(\"y train0:{0}\\ny test0:{1}\".format(y_train0.shape, y_test0.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input layer\n",
    "cifarInput = Input(shape=(x_train.shape[1:]), name=\"input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Small VGG-like model\n",
    "def simpleVGG(cifarInput, num_classes, name=\"vgg\"):\n",
    "    name = [name+str(i) for i in range(12)]\n",
    "    \n",
    "    # convolution and max pooling layers\n",
    "    vgg = Conv2D(32, (3, 3), padding='same', activation='relu', name=name[0])(cifarInput)\n",
    "    vgg = Conv2D(32, (3, 3), padding='same', activation='relu', name=name[1])(vgg)\n",
    "    vgg = MaxPooling2D(pool_size=(2,2), name=name[2])(vgg)\n",
    "    vgg = Dropout(0.25, name=name[3])(vgg)\n",
    "    vgg = Conv2D(64, (3, 3), padding='same', activation='relu', name=name[4])(vgg)\n",
    "    vgg = Conv2D(64, (3, 3), padding='same', activation='relu', name=name[5])(vgg)\n",
    "    vgg = MaxPooling2D(pool_size=(2,2), name=name[6])(vgg)\n",
    "    vgg = Dropout(0.25, name=name[7])(vgg)\n",
    "\n",
    "    # classification layers\n",
    "    vgg = Flatten(name=name[8])(vgg)\n",
    "    vgg = Dense(512, activation='relu', name=name[9])(vgg)\n",
    "    vgg = Dropout(0.5, name=name[10])(vgg)\n",
    "    vgg = Dense(num_classes, activation='softmax', name=name[11])(vgg)\n",
    "    return vgg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Large VGG-like model\n",
    "def fatVGG(cifarInput, num_classes, name=\"vgg\"):\n",
    "    name = [name+str(i) for i in range(17)]\n",
    "    \n",
    "    # convolution and max pooling layers\n",
    "    vgg = Conv2D(32, (3, 3), padding='same', activation='relu', name=name[0])(cifarInput)\n",
    "    vgg = Conv2D(32, (3, 3), padding='same', activation='relu', name=name[1])(vgg)\n",
    "    vgg = MaxPooling2D(pool_size=(2,2), name=name[2])(vgg)\n",
    "    vgg = Dropout(0.25, name=name[3])(vgg)\n",
    "    vgg = Conv2D(64, (3, 3), padding='same', activation='relu', name=name[4])(vgg)\n",
    "    vgg = Conv2D(64, (3, 3), padding='same', activation='relu', name=name[5])(vgg)\n",
    "    vgg = MaxPooling2D(pool_size=(2,2), name=name[6])(vgg)\n",
    "    vgg = Dropout(0.25, name=name[7])(vgg)\n",
    "    vgg = Conv2D(128, (3, 3), padding='same', activation='relu', name=name[8])(vgg)\n",
    "    vgg = Conv2D(128, (3, 3), padding='same', activation='relu', name=name[9])(vgg)\n",
    "    vgg = Conv2D(128, (3, 3), padding='same', activation='relu', name=name[10])(vgg)\n",
    "    vgg = MaxPooling2D(pool_size=(2,2), name=name[11])(vgg)\n",
    "    vgg = Dropout(0.25, name=name[12])(vgg)\n",
    "\n",
    "    # classification layers\n",
    "    vgg = Flatten(name=name[13])(vgg)\n",
    "    vgg = Dense(512, activation='relu', name=name[14])(vgg)\n",
    "    vgg = Dropout(0.5, name=name[15])(vgg)\n",
    "    vgg = Dense(num_classes, activation='softmax', name=name[16])(vgg)\n",
    "    return vgg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# first gating network, to decide artificial or natural object\n",
    "if use_large_gating_mlp:\n",
    "    gate0VGG = fatVGG(cifarInput, gate0_classes, \"gate0\")\n",
    "else:\n",
    "    gate0VGG = simpleVGG(cifarInput, gate0_classes, \"gate0\")\n",
    "gate0Model = Model(cifarInput, gate0VGG)\n",
    "\n",
    "# base VGG\n",
    "if use_large_baseline_classifier:\n",
    "    baseVGG = fatVGG(cifarInput, orig_classes, \"base\")\n",
    "else:\n",
    "    baseVGG = simpleVGG(cifarInput, orig_classes, \"base\") \n",
    "baseModel = Model(cifarInput, baseVGG)\n",
    "\n",
    "# artificial expert VGG\n",
    "if use_large_experts:\n",
    "    artificialVGG = fatVGG(cifarInput, orig_classes, \"artificial\")\n",
    "else:\n",
    "    artificialVGG = simpleVGG(cifarInput, orig_classes, \"artificial\")\n",
    "artificialModel = Model(cifarInput, artificialVGG)\n",
    "\n",
    "# naturalVGG = fatVGG(cifarInput, orig_classes, \"natural\")\n",
    "if use_large_experts:\n",
    "    naturalVGG = fatVGG(cifarInput, orig_classes, \"natural\")\n",
    "else:\n",
    "    naturalVGG = simpleVGG(cifarInput, orig_classes, \"natural\")\n",
    "\n",
    "naturalModel = Model(cifarInput, naturalVGG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Train networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train 10-Class Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compile\n",
    "baseModel.compile(loss='categorical_crossentropy',\n",
    "                   optimizer=Adam(),\n",
    "                   metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make saving directory for checkpoints\n",
    "baseSaveDir = \"./baseCifar10/\"\n",
    "if not os.path.isdir(baseSaveDir):\n",
    "    os.makedirs(baseSaveDir)\n",
    "    \n",
    "# early stopping and model checkpoint\n",
    "es_cb = EarlyStopping(monitor='val_loss', patience=2, verbose=1, mode='auto')\n",
    "chkpt = os.path.join(baseSaveDir, 'Cifar10_.{epoch:02d}-{val_loss:.2f}.hdf5')\n",
    "cp_cb = ModelCheckpoint(filepath = chkpt, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
    "\n",
    "# load the newest model data from the directory if exists\n",
    "baseModel = getNewestModel(baseModel, baseSaveDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train\n",
    "baseModel.fit(x_train, y_train0,\n",
    "               batch_size=batch_size,\n",
    "               epochs=epochs,\n",
    "               validation_data=(x_test, y_test0),\n",
    "               callbacks=[es_cb,cp_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# evaluate\n",
    "baseModel = getNewestModel(baseModel, baseSaveDir)\n",
    "baseScore = baseModel.evaluate(x_test, y_test0)\n",
    "print(baseScore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Train 2-Class Natural/Artificial Classifier\n",
    "\n",
    "The expert gating model determines whether an image is \"natural\" or \"artificial\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make ground truth for whether an example is \"natural\" or \"artificial\"\n",
    "y_trainG0 = np.array([0 if i in [0,1,8,9] else 1 for i in y_train])\n",
    "y_testG0 = np.array([0 if i in [0,1,8,9] else 1 for i in y_test])\n",
    "\n",
    "y_trainG0 = keras.utils.to_categorical(y_trainG0, 2)\n",
    "y_testG0  = keras.utils.to_categorical(y_testG0, 2)\n",
    "\n",
    "print(\"y trainG0:{0}\\ny testG0:{1}\".format(y_trainG0.shape, y_testG0.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compile\n",
    "gate0Model.compile(loss='categorical_crossentropy',\n",
    "                   optimizer=Adam(),\n",
    "                   metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make saving directory for check point\n",
    "gate0SaveDir = \"./gate0Cifar10/\"\n",
    "if not os.path.isdir(gate0SaveDir):\n",
    "    os.makedirs(gate0SaveDir)\n",
    "    \n",
    "# early stopping and model checkpoint\n",
    "es_cb = EarlyStopping(monitor='val_loss', patience=2, verbose=1, mode='auto')\n",
    "chkpt = os.path.join(gate0SaveDir, 'Cifar10_.{epoch:02d}-{val_loss:.2f}.hdf5')\n",
    "cp_cb = ModelCheckpoint(filepath = chkpt, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
    "\n",
    "# load the newest model data from the directory if exists\n",
    "gate0Model = getNewestModel(gate0Model, gate0SaveDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train\n",
    "gate0Model.fit(x_train, y_trainG0,\n",
    "               batch_size=batch_size,\n",
    "               epochs=epochs,\n",
    "               validation_data=(x_test, y_testG0),\n",
    "               callbacks=[es_cb,cp_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# evaluate\n",
    "gate0Model = getNewestModel(gate0Model, gate0SaveDir)\n",
    "gate0Score = gate0Model.evaluate(x_test, y_testG0)\n",
    "print(gate0Score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train \"Natural\" and \"Artificial\" Experts\n",
    "<br>\n",
    "The expert networks are specialized in predicting a certain classes.<br>\n",
    "Each network is only trained with its specialized field: the artificial expert get trained for labels 0, 1, 8 and 9; the natural expert for labels 2, 3, 4, 5, 6 and 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the position of artificial images and natural images in training and test dataset\n",
    "artTrain = [i for i in range(len(y_train)) if y_train[i] in [0,1,8,9]]\n",
    "natureTrain = [i for i in range(len(y_train)) if y_train[i] in [2,3,4,5,6,7]]\n",
    "artTest = [i for i in range(len(y_test)) if y_test[i] in [0,1,8,9]]\n",
    "natureTest = [i for i in range(len(y_test)) if y_test[i] in [2,3,4,5,6,7]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get artificial dataset and natural dataset\n",
    "x_trainArt = x_train[artTrain]\n",
    "x_testArt = x_test[artTest]\n",
    "y_trainArt = y_train[artTrain]\n",
    "y_testArt = y_test[artTest]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Artificial expert network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for artificial dataset\n",
    "y_trainArt = keras.utils.to_categorical(y_trainArt, orig_classes)\n",
    "y_testArt = keras.utils.to_categorical(y_testArt, orig_classes)\n",
    "\n",
    "print(\"y train art:{0}\\ny test art:{1}\".format(y_trainArt.shape, y_testArt.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compile\n",
    "artificialModel.compile(loss='categorical_crossentropy',\n",
    "                        optimizer=Adam(),\n",
    "                        metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make saving directory for check point\n",
    "artSaveDir = \"./artCifar10/\"\n",
    "if not os.path.isdir(artSaveDir):\n",
    "    os.makedirs(artSaveDir)\n",
    "    \n",
    "# early stopping and model checkpoint\n",
    "es_cb = EarlyStopping(monitor='val_loss', patience=2, verbose=1, mode='auto')\n",
    "chkpt = os.path.join(artSaveDir, 'Cifar10_.{epoch:02d}-{val_loss:.2f}.hdf5')\n",
    "cp_cb = ModelCheckpoint(filepath = chkpt, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
    "\n",
    "# load the newest model data if exists\n",
    "artificialModel = getNewestModel(artificialModel, artSaveDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train\n",
    "artificialModel.fit(x_trainArt, y_trainArt,\n",
    "               batch_size=batch_size,\n",
    "               epochs=epochs,\n",
    "               validation_data=(x_testArt, y_testArt),\n",
    "               callbacks=[es_cb,cp_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# evaluate\n",
    "artificialModel = getNewestModel(artificialModel, artSaveDir)\n",
    "artScore = artificialModel.evaluate(x_testArt, y_testArt)\n",
    "print(artScore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Natural expert network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for natural dataset\n",
    "x_trainNat = x_train[natureTrain]\n",
    "x_testNat = x_test[natureTest]\n",
    "y_trainNat = y_train[natureTrain]\n",
    "y_testNat = y_test[natureTest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get natural dataset\n",
    "y_trainNat = keras.utils.to_categorical(y_trainNat, orig_classes)\n",
    "y_testNat = keras.utils.to_categorical(y_testNat, orig_classes)\n",
    "\n",
    "print(\"y train nature:{0}\\ny test nature:{1}\".format(y_trainNat.shape, y_testNat.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compile\n",
    "naturalModel.compile(loss='categorical_crossentropy',\n",
    "                   optimizer=Adam(),\n",
    "                   metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make saving directory for check point\n",
    "natSaveDir = \"./natureCifar10/\"\n",
    "if not os.path.isdir(natSaveDir):\n",
    "    os.makedirs(natSaveDir)\n",
    "    \n",
    "# early stopping and model checkpoint\n",
    "es_cb = EarlyStopping(monitor='val_loss', patience=2, verbose=1, mode='auto')\n",
    "chkpt = os.path.join(natSaveDir, 'Cifar10_.{epoch:02d}-{val_loss:.2f}.hdf5')\n",
    "cp_cb = ModelCheckpoint(filepath = chkpt, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
    "\n",
    "# load the newest model data if exists\n",
    "naturalModel = getNewestModel(naturalModel, natSaveDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train\n",
    "naturalModel.fit(x_trainNat, y_trainNat,\n",
    "               batch_size=batch_size,\n",
    "               epochs=epochs,\n",
    "               validation_data=(x_testNat, y_testNat),\n",
    "               callbacks=[es_cb,cp_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# evaluate\n",
    "naturalModel = getNewestModel(naturalModel, natSaveDir)\n",
    "natScore = naturalModel.evaluate(x_testNat, y_testNat)\n",
    "print(natScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freeze the weights of all trained models so far (i.e. baseline, experts, and expert gating models).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for l in baseModel.layers:\n",
    "    l.trainable = False\n",
    "for l in gate0Model.layers:\n",
    "    l.trainable = False\n",
    "for l in artificialModel.layers:\n",
    "    l.trainable = False\n",
    "for l in naturalModel.layers:\n",
    "    l.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Connecting the overall networks to form the mixture of experts model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define sub-Gate network, for the second gating network layer\n",
    "def subGate(cifarInput, orig_classes, numExperts, name=\"subGate\"):\n",
    "    name = [name+str(i) for i in range(5)]\n",
    "    subgate = Flatten(name=name[0])(cifarInput)\n",
    "    subgate = Dense(512, activation='relu', name=name[1])(subgate)\n",
    "    subgate = Dropout(0.5, name=name[2])(subgate)\n",
    "    subgate = Dense(orig_classes*numExperts, activation='softmax', name=name[3])(subgate)\n",
    "    subgate = Reshape((orig_classes, numExperts), name=name[4])(subgate)\n",
    "    return subgate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the artificial gating network\n",
    "artGate = subGate(cifarInput, orig_classes, 2, \"artExpertGate\")\n",
    "\n",
    "# the natural gating network\n",
    "natureGate = subGate(cifarInput, orig_classes, 2, \"natureExpertGate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define inference calculation with Keras Lambda layer with base VGG, expert network and the second gating network of corresponding expert as input\n",
    "# the inference is calculated as sum of multiplications of base VGG inference output and its importance, and expert network inference output and its importance\n",
    "def subGateLambda(base, expert, subgate):\n",
    "    output = Lambda(lambda gx: (gx[0]*gx[2][:,:,0]) + (gx[1]*gx[2][:,:,1]), output_shape=(orig_classes,))([base, expert, subgate])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# connecting the overall networks.\n",
    "# the Keras backend switch works as deciding with the first gating network, leading to artificial or natural gate\n",
    "output = Lambda(lambda gx: K.switch(gx[1][:,0] > gx[1][:,1], \n",
    "                                    subGateLambda(gx[0], gx[2], gx[4]), \n",
    "                                    subGateLambda(gx[0], gx[3], gx[5])), \n",
    "                output_shape=(orig_classes,))([baseVGG, gate0VGG, artificialVGG, naturalVGG, artGate, natureGate])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# the mixture of experts model\n",
    "model = Model(cifarInput, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compile\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# show layers and if it's trainable or not\n",
    "# only the second gating network layers and the last Lambda inference layer are left trainable\n",
    "for l in model.layers:\n",
    "    print(l, l.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make saving directory for check point\n",
    "saveDir = \"./moe3Cifar10/\"\n",
    "if not os.path.isdir(saveDir):\n",
    "    os.makedirs(saveDir)\n",
    "    \n",
    "# early stopping and model checkpoint\n",
    "es_cb = EarlyStopping(monitor='val_loss', patience=2, verbose=1, mode='auto')\n",
    "chkpt = os.path.join(saveDir, 'Cifar10_.{epoch:02d}-{val_loss:.2f}.hdf5')\n",
    "cp_cb = ModelCheckpoint(filepath = chkpt, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
    "\n",
    "# load the newest model data if exists\n",
    "model = getNewestModel(model, saveDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train\n",
    "model.fit(x_train, y_train0,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test, y_test0),\n",
    "          callbacks=[es_cb, cp_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# evaluate\n",
    "mixture_loss_accuracy = model.evaluate(x_test, y_test0)\n",
    "print(mixture_loss_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
